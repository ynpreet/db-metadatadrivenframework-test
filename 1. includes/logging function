Great! Since you‚Äôre using %run to execute the function, you don‚Äôt need sys.path.append(). Instead, you‚Äôll store your logging_utils.py in a Databricks notebook inside your Git repository and call it using %run.

1Ô∏è‚É£ Create a Shared Logging Notebook
	1.	Inside your Git repo in Databricks, create a new notebook.
	2.	Name it Logging_Utils.
	3.	Paste this code inside Logging_Utils:

# Logging Utilities for Databricks Notebooks
import logging
import os
import datetime

def setup_logger():
    """
    Initializes a shared logger for all notebooks.
    Creates a daily log file in the /tmp/ directory.
    """
    logfile_prefix = "my_logger"
    file_date = datetime.datetime.now().strftime("%Y-%m-%d")
    log_file_name = f"{logfile_prefix}_{file_date}.log"
    log_dir = "/tmp/"  # Local storage in Databricks
    log_file = os.path.join(log_dir, log_file_name)

    # Create logger
    logger = logging.getLogger("custom_log")
    logger.setLevel(logging.DEBUG)

    # Check if handlers already exist to avoid duplicate logs
    if not logger.handlers:
        file_handler = logging.FileHandler(log_file, mode="a")
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter("%(levelname)s:%(asctime)s:%(message)s")
        file_handler.setFormatter(formatter)
        stream_handler.setFormatter(formatter)

        # Add handlers
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)

    return logger, log_file

def push_logs_to_storage():
    """
    Moves the daily log file from /tmp/ to Azure Data Lake Storage (ADLS).
    """
    # Define storage location
    storage_path = "abfss://application-logs@<storage_account>.dfs.core.windows.net/logs/"

    # Use the same file name as generated in setup_logger()
    logfile_prefix = "my_logger"
    file_date = datetime.datetime.now().strftime("%Y-%m-%d")
    log_file_name = f"{logfile_prefix}_{file_date}.log"
    local_log_path = f"/tmp/{log_file_name}"
    adls_path = f"{storage_path}{log_file_name}"

    # Move log file to storage
    try:
        dbutils.fs.mv(f"file:{local_log_path}", adls_path, True)
        print(f"Log file successfully moved to: {adls_path}")
    except Exception as e:
        print(f"Error moving log file: {str(e)}")

2Ô∏è‚É£ Call the Logging Notebook in Your Notebooks

At the top of every notebook, use %run to import the logging functions:

# Run the shared logging notebook
%run "/Repos/your-git-repo/Logging_Utils"

# Initialize logger
logger, log_file = setup_logger()
logger.info("Notebook execution started")

üîπ What Happens Here?
	‚Ä¢	%run "/Repos/your-git-repo/Logging_Utils" loads the logging functions from Logging_Utils notebook.
	‚Ä¢	setup_logger() ensures all notebooks log to the same file.

3Ô∏è‚É£ Push Logs to Storage at the End

At the end of the last notebook, move logs to Azure Storage:

push_logs_to_storage()

4Ô∏è‚É£ Commit to Git

Since you are using a Git repo, commit your changes:

git add Logging_Utils
git commit -m "Added shared logging notebook"
git push origin main

Now, whenever you clone the repo on a new cluster, your logging setup will work seamlessly.

‚úÖ Why This Approach Works?

‚úÖ No need for sys.path.append() (Since %run directly executes the notebook).
‚úÖ All notebooks share the same logger and log file per day.
‚úÖ Easy to maintain, modify, and version-control inside Git.
‚úÖ Automatically pushes logs to Azure Storage at the end.

Now your logs are structured, centralized, and accessible across all notebooks! üöÄ Let me know if you need any refinements.






The global log file name should be consistent across all notebooks and should use a single timestamp generated at the start of execution. The best practice is:

Global Log File Naming Convention:

my_logger_YYYY-MM-DD_HH-MM-SS.log

For example:

my_logger_2025-01-28_14-30-15.log

	‚Ä¢	Prefix (my_logger) ‚Üí Identifies that it‚Äôs a log file.
	‚Ä¢	Date (YYYY-MM-DD) ‚Üí Ensures logs are organized daily.
	‚Ä¢	Time (HH-MM-SS) ‚Üí Helps differentiate log files if multiple runs happen on the same day.

üîπ How to Ensure a Single Global Log File Name

Since multiple notebooks execute separately, we need a single timestamp stored globally.

‚úÖ Store the Log File Name in a Global Variable

Modify Logging_Utils to generate the filename once and store it in GLOBAL_LOG_FILE:

import logging
import os
import datetime

# Generate timestamp ONCE when the module is loaded
GLOBAL_TIMESTAMP = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
GLOBAL_LOG_FILE = f"my_logger_{GLOBAL_TIMESTAMP}.log"

def setup_logger():
    """
    Initializes a shared logger for all notebooks.
    Uses a globally consistent log file.
    """
    log_dir = "/tmp/"  # Local storage in Databricks
    log_file = os.path.join(log_dir, GLOBAL_LOG_FILE)  # Use global filename

    # Create logger
    logger = logging.getLogger("custom_log")
    logger.setLevel(logging.DEBUG)

    # Check if handlers already exist to avoid duplicate logs
    if not logger.handlers:
        file_handler = logging.FileHandler(log_file, mode="a")
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter("%(levelname)s:%(asctime)s:%(message)s")
        file_handler.setFormatter(formatter)
        stream_handler.setFormatter(formatter)

        # Add handlers
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)

    return logger, log_file, GLOBAL_LOG_FILE  # Return the consistent log filename

def push_logs_to_storage():
    """
    Moves the log file from /tmp/ to Azure Data Lake Storage (ADLS).
    Uses the globally consistent log filename.
    """
    storage_path = "abfss://application-logs@<storage_account>.dfs.core.windows.net/logs/"

    local_log_path = f"/tmp/{GLOBAL_LOG_FILE}"
    adls_path = f"{storage_path}{GLOBAL_LOG_FILE}"

    # Move log file to storage
    try:
        dbutils.fs.mv(f"file:{local_log_path}", adls_path, True)
        print(f"Log file successfully moved to: {adls_path}")
    except Exception as e:
        print(f"Error moving log file: {str(e)}")

üîπ How to Use This in Your Notebooks

1Ô∏è‚É£ Initialize Logger in Every Notebook

At the top of each notebook, run:

# Load the shared logging functions
%run "/Repos/your-git-repo/Logging_Utils"

# Initialize the logger (uses a global log file)
logger, log_file, GLOBAL_LOG_FILE = setup_logger()

logger.info(f"Notebook execution started. Logging to: {GLOBAL_LOG_FILE}")

2Ô∏è‚É£ Push Logs to Storage at the End

At the end of the last notebook, move the logs to Azure:

push_logs_to_storage()

üîπ How This Ensures a Consistent Log File
	‚Ä¢	All notebooks use the same GLOBAL_LOG_FILE name.
	‚Ä¢	Prevents mismatches in filenames when pushing logs to storage.
	‚Ä¢	Every execution session gets a unique log file (identified by timestamp).

‚úÖ Now, all logs across notebooks will be stored in the same file, ensuring consistency! üöÄ Let me know if you need any refinements.







You‚Äôre correct! Since the logging utility notebook (Logging_Utils) is executed at the top of every notebook, the GLOBAL_TIMESTAMP is regenerated each time, leading to different log filenames.

We need to persist the timestamp across all notebooks so that:
	1.	All notebooks use the same timestamp for logging.
	2.	The log filename remains consistent across the workflow.
	3.	The timestamp is only generated once per workflow execution.

‚úÖ Solution: Store the Timestamp in a Databricks Widget or a Temporary Table

Approach 1: Use Databricks Widget (Recommended)
	1.	Create the timestamp once and store it in a Databricks widget.
	2.	Retrieve the timestamp from the widget in every notebook to ensure consistency.

1Ô∏è‚É£ Modify the Logging_Utils Notebook to Initialize Only Once

Modify your Logging_Utils notebook to generate the timestamp only if it doesn‚Äôt already exist.

import logging
import os
import datetime
import pyspark.sql.functions as F

# Check if the timestamp is already stored in a Databricks widget
try:
    GLOBAL_TIMESTAMP = dbutils.widgets.get("GLOBAL_TIMESTAMP")
except:
    # If widget doesn't exist, generate it once
    GLOBAL_TIMESTAMP = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    dbutils.widgets.text("GLOBAL_TIMESTAMP", GLOBAL_TIMESTAMP)  # Store it in Databricks widget

# Define global log filename
GLOBAL_LOG_FILE = f"my_logger_{GLOBAL_TIMESTAMP}.log"

def setup_logger():
    """
    Initializes a shared logger for all notebooks.
    Uses a globally consistent log file by reading the timestamp from the widget.
    """
    log_dir = "/tmp/"  # Local storage in Databricks
    log_file = os.path.join(log_dir, GLOBAL_LOG_FILE)  # Use global filename

    # Create logger
    logger = logging.getLogger("custom_log")
    logger.setLevel(logging.DEBUG)

    # Check if handlers already exist to avoid duplicate logs
    if not logger.handlers:
        file_handler = logging.FileHandler(log_file, mode="a")
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter("%(levelname)s:%(asctime)s:%(message)s")
        file_handler.setFormatter(formatter)
        stream_handler.setFormatter(formatter)

        # Add handlers
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)

    return logger, log_file, GLOBAL_LOG_FILE  # Return the consistent log filename

def push_logs_to_storage():
    """
    Moves the log file from /tmp/ to Azure Data Lake Storage (ADLS).
    Uses the globally consistent log filename stored in the widget.
    """
    storage_path = "abfss://application-logs@<storage_account>.dfs.core.windows.net/logs/"

    local_log_path = f"/tmp/{GLOBAL_LOG_FILE}"
    adls_path = f"{storage_path}{GLOBAL_LOG_FILE}"

    # Move log file to storage
    try:
        dbutils.fs.mv(f"file:{local_log_path}", adls_path, True)
        print(f"Log file successfully moved to: {adls_path}")
    except Exception as e:
        print(f"Error moving log file: {str(e)}")

2Ô∏è‚É£ Use This in Your Notebooks

At the top of every notebook, use:

# Load the shared logging functions
%run "/Repos/your-git-repo/Logging_Utils"

# Retrieve the timestamp from the widget
GLOBAL_TIMESTAMP = dbutils.widgets.get("GLOBAL_TIMESTAMP")

# Initialize the logger (uses the same global log file across all notebooks)
logger, log_file, GLOBAL_LOG_FILE = setup_logger()

logger.info(f"Notebook execution started. Logging to: {GLOBAL_LOG_FILE}")

3Ô∏è‚É£ Push Logs to Storage at the End

At the end of the last notebook, move the logs to Azure:

push_logs_to_storage()

4Ô∏è‚É£ Verify the Global Timestamp (Optional)

If you want to manually check the timestamp, run:

print(dbutils.widgets.get("GLOBAL_TIMESTAMP"))

‚úÖ Why This Works?
	‚Ä¢	Ensures all notebooks use the same timestamp for logging ‚úÖ
	‚Ä¢	Log filename remains consistent across the entire workflow ‚úÖ
	‚Ä¢	The timestamp is generated only once per workflow execution ‚úÖ
	‚Ä¢	Persists across multiple notebooks without redefining variables ‚úÖ

This approach solves the problem completely and ensures consistent logging across all Databricks notebooks. üöÄ Let me know if you need any modifications!




