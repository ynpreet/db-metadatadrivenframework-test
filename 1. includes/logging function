Great! Since you‚Äôre using %run to execute the function, you don‚Äôt need sys.path.append(). Instead, you‚Äôll store your logging_utils.py in a Databricks notebook inside your Git repository and call it using %run.

1Ô∏è‚É£ Create a Shared Logging Notebook
	1.	Inside your Git repo in Databricks, create a new notebook.
	2.	Name it Logging_Utils.
	3.	Paste this code inside Logging_Utils:

# Logging Utilities for Databricks Notebooks
import logging
import os
import datetime

def setup_logger():
    """
    Initializes a shared logger for all notebooks.
    Creates a daily log file in the /tmp/ directory.
    """
    logfile_prefix = "my_logger"
    file_date = datetime.datetime.now().strftime("%Y-%m-%d")
    log_file_name = f"{logfile_prefix}_{file_date}.log"
    log_dir = "/tmp/"  # Local storage in Databricks
    log_file = os.path.join(log_dir, log_file_name)

    # Create logger
    logger = logging.getLogger("custom_log")
    logger.setLevel(logging.DEBUG)

    # Check if handlers already exist to avoid duplicate logs
    if not logger.handlers:
        file_handler = logging.FileHandler(log_file, mode="a")
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter("%(levelname)s:%(asctime)s:%(message)s")
        file_handler.setFormatter(formatter)
        stream_handler.setFormatter(formatter)

        # Add handlers
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)

    return logger, log_file

def push_logs_to_storage():
    """
    Moves the daily log file from /tmp/ to Azure Data Lake Storage (ADLS).
    """
    # Define storage location
    storage_path = "abfss://application-logs@<storage_account>.dfs.core.windows.net/logs/"

    # Use the same file name as generated in setup_logger()
    logfile_prefix = "my_logger"
    file_date = datetime.datetime.now().strftime("%Y-%m-%d")
    log_file_name = f"{logfile_prefix}_{file_date}.log"
    local_log_path = f"/tmp/{log_file_name}"
    adls_path = f"{storage_path}{log_file_name}"

    # Move log file to storage
    try:
        dbutils.fs.mv(f"file:{local_log_path}", adls_path, True)
        print(f"Log file successfully moved to: {adls_path}")
    except Exception as e:
        print(f"Error moving log file: {str(e)}")

2Ô∏è‚É£ Call the Logging Notebook in Your Notebooks

At the top of every notebook, use %run to import the logging functions:

# Run the shared logging notebook
%run "/Repos/your-git-repo/Logging_Utils"

# Initialize logger
logger, log_file = setup_logger()
logger.info("Notebook execution started")

üîπ What Happens Here?
	‚Ä¢	%run "/Repos/your-git-repo/Logging_Utils" loads the logging functions from Logging_Utils notebook.
	‚Ä¢	setup_logger() ensures all notebooks log to the same file.

3Ô∏è‚É£ Push Logs to Storage at the End

At the end of the last notebook, move logs to Azure Storage:

push_logs_to_storage()

4Ô∏è‚É£ Commit to Git

Since you are using a Git repo, commit your changes:

git add Logging_Utils
git commit -m "Added shared logging notebook"
git push origin main

Now, whenever you clone the repo on a new cluster, your logging setup will work seamlessly.

‚úÖ Why This Approach Works?

‚úÖ No need for sys.path.append() (Since %run directly executes the notebook).
‚úÖ All notebooks share the same logger and log file per day.
‚úÖ Easy to maintain, modify, and version-control inside Git.
‚úÖ Automatically pushes logs to Azure Storage at the end.

Now your logs are structured, centralized, and accessible across all notebooks! üöÄ Let me know if you need any refinements.






The global log file name should be consistent across all notebooks and should use a single timestamp generated at the start of execution. The best practice is:

Global Log File Naming Convention:

my_logger_YYYY-MM-DD_HH-MM-SS.log

For example:

my_logger_2025-01-28_14-30-15.log

	‚Ä¢	Prefix (my_logger) ‚Üí Identifies that it‚Äôs a log file.
	‚Ä¢	Date (YYYY-MM-DD) ‚Üí Ensures logs are organized daily.
	‚Ä¢	Time (HH-MM-SS) ‚Üí Helps differentiate log files if multiple runs happen on the same day.

üîπ How to Ensure a Single Global Log File Name

Since multiple notebooks execute separately, we need a single timestamp stored globally.

‚úÖ Store the Log File Name in a Global Variable

Modify Logging_Utils to generate the filename once and store it in GLOBAL_LOG_FILE:

import logging
import os
import datetime

# Generate timestamp ONCE when the module is loaded
GLOBAL_TIMESTAMP = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
GLOBAL_LOG_FILE = f"my_logger_{GLOBAL_TIMESTAMP}.log"

def setup_logger():
    """
    Initializes a shared logger for all notebooks.
    Uses a globally consistent log file.
    """
    log_dir = "/tmp/"  # Local storage in Databricks
    log_file = os.path.join(log_dir, GLOBAL_LOG_FILE)  # Use global filename

    # Create logger
    logger = logging.getLogger("custom_log")
    logger.setLevel(logging.DEBUG)

    # Check if handlers already exist to avoid duplicate logs
    if not logger.handlers:
        file_handler = logging.FileHandler(log_file, mode="a")
        stream_handler = logging.StreamHandler()
        stream_handler.setLevel(logging.DEBUG)

        formatter = logging.Formatter("%(levelname)s:%(asctime)s:%(message)s")
        file_handler.setFormatter(formatter)
        stream_handler.setFormatter(formatter)

        # Add handlers
        logger.addHandler(file_handler)
        logger.addHandler(stream_handler)

    return logger, log_file, GLOBAL_LOG_FILE  # Return the consistent log filename

def push_logs_to_storage():
    """
    Moves the log file from /tmp/ to Azure Data Lake Storage (ADLS).
    Uses the globally consistent log filename.
    """
    storage_path = "abfss://application-logs@<storage_account>.dfs.core.windows.net/logs/"

    local_log_path = f"/tmp/{GLOBAL_LOG_FILE}"
    adls_path = f"{storage_path}{GLOBAL_LOG_FILE}"

    # Move log file to storage
    try:
        dbutils.fs.mv(f"file:{local_log_path}", adls_path, True)
        print(f"Log file successfully moved to: {adls_path}")
    except Exception as e:
        print(f"Error moving log file: {str(e)}")

üîπ How to Use This in Your Notebooks

1Ô∏è‚É£ Initialize Logger in Every Notebook

At the top of each notebook, run:

# Load the shared logging functions
%run "/Repos/your-git-repo/Logging_Utils"

# Initialize the logger (uses a global log file)
logger, log_file, GLOBAL_LOG_FILE = setup_logger()

logger.info(f"Notebook execution started. Logging to: {GLOBAL_LOG_FILE}")

2Ô∏è‚É£ Push Logs to Storage at the End

At the end of the last notebook, move the logs to Azure:

push_logs_to_storage()

üîπ How This Ensures a Consistent Log File
	‚Ä¢	All notebooks use the same GLOBAL_LOG_FILE name.
	‚Ä¢	Prevents mismatches in filenames when pushing logs to storage.
	‚Ä¢	Every execution session gets a unique log file (identified by timestamp).

‚úÖ Now, all logs across notebooks will be stored in the same file, ensuring consistency! üöÄ Let me know if you need any refinements.







