import logging
import time
import datetime
import os

# Generate timestamp for the log file name
file_date = datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d-%H-%M-%S')

# Define the directories
dbfs_dir = "/dbfs/tmp/logs/"  # Temporary DBFS directory for logs
workspace_dir = "/Workspace/Users/ynpreetinusa@gmail.com/Formulal1onmyown/includes/logging/"  # Workspace directory for logs

# Ensure the DBFS directory exists
os.makedirs(dbfs_dir, exist_ok=True)

# Define the log file path
p_filename = f"custom_log_{file_date}.log"
p_logfile = os.path.join(dbfs_dir, p_filename)

# Print the log file path
print(p_logfile)

# Create logger
logger = logging.getLogger("Custom_Log")
logger.setLevel(logging.DEBUG)

# Create file handler and set formatter
fh = logging.FileHandler(p_logfile, mode="w")
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
fh.setFormatter(formatter)
logger.addHandler(fh)

# Test logging
logger.info("I am now working")

# Copy the log file to the workspace directory
try:
    dbutils.fs.cp(f"file:{p_logfile}", f"{workspace_dir}{p_filename}")
    print(f"Log file copied to workspace directory: {workspace_dir}{p_filename}")
except Exception as e:
    print(f"Error copying log file to workspace: {str(e)}")



display(spark.read.text('/Workspace/Users/ynpreetinusa@gmail.com/Formulal1onmyown/includes/logging/custom_log_2025-01-27-22-42-48.log'))







import logging
from azure.storage.fileshare import ShareFileClient

# Define your ADLS Gen2 path
adls_dir = "abfss://logging@yourstorageaccount.dfs.core.windows.net/yourcontainer"

# Generate the log file path
file_date = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
log_filename = f"custom_log_{file_date}.log"
adls_log_path = f"{adls_dir}/{log_filename}"

# Set up the logger
logger = logging.getLogger("Custom_Log")
logger.setLevel(logging.DEBUG)

# Use a file handler with the ADLS path
fh = logging.FileHandler(adls_log_path, mode="w")
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
fh.setFormatter(formatter)
logger.addHandler(fh)

# Log some messages
logger.info("Logging directly to ADLS Gen2 started")
logger.debug("This is a debug message")
logger.error("This is an error message")

# Close and flush the file handler
fh.close()
logger.removeHandler(fh)




import logging
import datetime
from azure.storage.blob import BlobServiceClient

# Define your ADLS Gen2 credentials
connection_string = "your_connection_string"
container_name = "your_container_name"
folder_name = "logs"

# Generate log file name
file_date = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')
log_filename = f"custom_log_{file_date}.log"
local_log_path = f"/dbfs/tmp/{log_filename}"

# Set up the logger
logger = logging.getLogger("Custom_Log")
logger.setLevel(logging.DEBUG)

# Configure file handler to write logs locally
fh = logging.FileHandler(local_log_path, mode="w")
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
fh.setFormatter(formatter)
logger.addHandler(fh)

# Log some messages
logger.info("Logging directly to ADLS Gen2 started")
logger.debug("This is a debug message")
logger.error("This is an error message")

# Close the file handler
fh.close()
logger.removeHandler(fh)

# Upload the log file to ADLS Gen2
blob_service_client = BlobServiceClient.from_connection_string(connection_string)
blob_client = blob_service_client.get_blob_client(
    container=container_name, blob=f"{folder_name}/{log_filename}"
)

with open(local_log_path, "rb") as data:
    blob_client.upload_blob(data, overwrite=True)

print(f"Log file uploaded to ADLS Gen2 at {folder_name}/{log_filename}")
